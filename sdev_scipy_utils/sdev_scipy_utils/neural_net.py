import tensorflow as tf

# Input: 29 -> encode: 100 -> latent -> decode: 100 -> Output: 29.


class AE(tf.keras.Model):
    def __init__(self, latent_dim):
        super(AE, self).__init__()
        self.latent_dim = latent_dim
        self.encoder = tf.keras.Sequential(
            [
                tf.keras.layers.InputLayer(input_shape=(None, 118), name="InputLayer"),
                tf.keras.layers.Dense(
                    100,
                    kernel_initializer="uniform",
                    activation="tanh",
                    name="Encoder_1",
                ),
                tf.keras.layers.Dense(
                    latent_dim,
                    kernel_initializer="uniform",
                    activation="tanh",
                    name="Laten_Space",
                ),
            ],
            name="Encoder",
        )

        self.decoder = tf.keras.Sequential(
            [
                tf.keras.layers.InputLayer(input_shape=(latent_dim,)),
                tf.keras.layers.Dense(
                    100, kernel_initializer="uniform", activation="tanh"
                ),
                tf.keras.layers.Dense(
                    118, kernel_initializer="uniform", activation="linear"
                ),
            ],
            name="Decoder",
        )
        self.AE_model = tf.keras.Model(
            inputs=self.encoder.input,
            outputs=self.decoder(self.encoder.output),
            name="Auto Encoder",
        )

    def call(self, input_tensor):
        latent_space = self.encoder.output
        reconstruction = self.decoder(latent_space)
        AE_model = tf.keras.Model(inputs=self.encoder.input, outputs=reconstruction)
        return AE_model(input_tensor)

    def summary(self):
        return self.AE_model.summary()


def mse_reconstruction(model, X_train):
    """
    * type-def ::(Any, np.ndarray) -> pd.DataFrame
    * ---------------{Function}---------------
        * Calculates the mean squared error (MSE) of the reconstructed data and returns a DataFrame with reconstruction error and true class.
    * ----------------{Returns}---------------
        * : train_error ::pd.DataFrame | A DataFrame containing the reconstruction error and true class for each data point
    * ----------------{Params}----------------
        * : model ::Any | The trained autoencoder model
        * : X_train ::np.ndarray | The training data
    * ----------------{Usage}-----------------
        * >>> from tensorflow.keras.models import Model
        * >>> # Assume we have a trained autoencoder model 'autoencoder'
        * >>> X_train = np.random.randn(100, 2)
        * >>> train_error = mse_reconstruction(autoencoder, X_train)
    * ----------------{Notes}-----------------
        * This function is used for detecting anomalies by computing the MSE between the original training data and the reconstructed data generated by the autoencoder model.
        * Mean squared error, identify the reconstruction error in order to detect anomalies
    """
    train_mse = np.mean(np.power(model(X_train) - X_train, 2), axis=1)
    train_error = pd.DataFrame(
        {"Reconstruction_error": train_mse, "True_class": y_train}
    )
    return train_error


def AE_predictor(X, model, threshold):
    """
    * type-def ::(np.ndarray, Any, float) -> np.ndarray
    * ---------------{Function}---------------
        * Given a dataset, a trained autoencoder model, and a threshold, predicts whether the data points are anomalies or not.
    * ----------------{Returns}---------------
        * : y ::np.ndarray | A binary array where 1 denotes an anomaly, and 0 denotes a normal data point
    * ----------------{Params}----------------
        * : X ::np.ndarray | The dataset to be classified
        * : model ::Any | The trained autoencoder model
        * : threshold ::float | The MSE threshold value for classifying a data point as an anomaly
    * ----------------{Usage}-----------------
        * >>> from tensorflow.keras.models import Model
        * >>> # Assume we have a trained autoencoder model 'autoencoder'
        * >>> X = np.random.randn(50, 2)
        * >>> threshold = 0.5
        * >>> y_pred = AE_predictor(X, autoencoder, threshold)
    * ----------------{Notes}-----------------
        * This function helps classify data points as anomalies or normal instances based on the MSE threshold. If the MSE between the original data and the reconstructed data is greater than the threshold, the data point is considered an anomaly.
        * if mse > threshold, then we can classify this as an anomaly
    """
    X_valid = model(X)
    mse = np.mean(np.power(X_valid - X, 2), axis=1)
    y = np.zeros(shape=mse.shape)
    y[mse > threshold] = 1
    return y


def plot_confusion_matrix(y_test, y_pred):
    from sklearn.metrics import confusion_matrix, classification_report

    cm = confusion_matrix(y_test, y_pred)
    print(classification_report(y_test, y_pred))

    plt.figure()
    sns.heatmap(cm, cmap="coolwarm", annot=True, linewidths=0.5)
    plt.title("Confusion Matrix")
    plt.xlabel("Predicted class")
    plt.ylabel("Real class")
    plt.show()


#

import torch
import math
import numpy as np
import torch.optim as optim
import pandas as pd
from torch import nn
from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score
from collections import defaultdict


def set_device(gpu=-1):
    if gpu != -1 and torch.cuda.is_available():
        device = torch.device("cuda: " + str(gpu))
    else:
        device = torch.device("cpu")
    return device


def umap_reshape(data, enclose=False):
    """
    * type-def ::(np.ndarray, bool) -> np.ndarray
    * ---------------{Function}---------------
        * Reshapes an input data array by collapsing dimensions.
    * ----------------{Returns}---------------
        * : reshaped_data ::np.ndarray | The reshaped input data
    * ----------------{Params}----------------
        * : data ::np.ndarray | The input data to be reshaped
        * : enclose ::bool | If True, enclose the reshaped data in a new array; default is False
    * ----------------{Usage}-----------------
        * >>> import numpy as np
        * >>> data = np.random.randn(4, 4, 4)
        * >>> reshaped_data = umap_reshape(data)
    * ----------------{Notes}-----------------
        * This function helps in reshaping the input data by collapsing the dimensions.
        *It is useful for preparing data for dimensionality reduction techniques like UMAP.
    """
    c_shape = data.shape
    result = []
    for i in range(len(c_shape)):
        if i == 0 and c_shape[i] > 1:
            result.append(c_shape[i])
        else:
            pass
        if i > 0:
            result.append(c_shape[i])
    final_result = 1
    for i in range(len(result)):
        final_result = final_result * result[i]
    if enclose:
        return data.reshape(1, final_result)
    else:
        return data.reshape(final_result)


def max_len_pad(df, _dtype=np.int32):
    """
    * type-def ::(pd.Series, Any) -> pd.Series
    * ---------------{Function}---------------
        * Pads arrays in a pandas Series to have the same length by filling with zeros.
    * ----------------{Returns}---------------
        * : result_df ::pd.Series | A pandas Series with the arrays padded to have the same length
    * ----------------{Params}----------------
        * : df ::pd.Series | The input pandas Series containing arrays to be padded
        * : _dtype ::Any | The data type to be used for padding, default is np.int32
    * ----------------{Usage}-----------------
        * >>> import pandas as pd
        * >>> data = [np.array([1, 2]), np.array([1, 2, 3]), np.array([1, 2, 3, 4])]
        * >>> df = pd.Series(data)
        * >>> result_df = max_len_pad(df)
    * ----------------{Notes}-----------------
        * This function pads the arrays in a pandas Series to have the same length by filling with zeros.
        * It is useful when you need to have the same length for all arrays before performing further operations.
    """
    import numpy as np

    max_len = max(map(len, df))

    result_df = df.apply(
        lambda x: np.concatenate([x, np.zeros((max_len - x.shape[0]), dtype=_dtype)])
    )

    return result_df


def encode_tokenize(df, col, tokenizer):
    """
    * type-def ::(pd.DataFrame, str, transformers.PreTrainedTokenizer) -> pd.Series
    * ---------------{Function}---------------
        * Tokenizes and encodes the text data in a DataFrame column using a specified tokenizer.
    * ----------------{Returns}---------------
        * : digits ::pd.Series | A pandas Series containing the tokenized and encoded text data
    * ----------------{Params}----------------
        * : df ::pd.DataFrame | The input DataFrame containing the text data to be tokenized and encoded
        * : col ::str | The column name of the text data in the DataFrame
        * : tokenizer ::transformers.PreTrainedTokenizer | The tokenizer to be used for tokenization and encoding
    * ----------------{Usage}-----------------
        * >>> from transformers import AutoTokenizer
        * >>> tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
        * >>> data = {"text": ["Hello, world!", "This is an example."]}
        * >>> df = pd.DataFrame(data)
        * >>> digits = encode_tokenize(df, "text", tokenizer)
    * ----------------{Notes}-----------------
        * This function tokenizes and encodes text data in a DataFrame column using a specified tokenizer from the transformers library.
    """
    import torch

    digits = df[col].fillna("nn").apply(lambda x: torch.tensor([tokenizer.encode(x)]))
    return digits


def embed_tokens_bert(digits, bert_model):
    """
    * type-def ::(pd.Series, transformers.PreTrainedModel) -> pd.Series
    * ---------------{Function}---------------
        * Embeds tokenized text data using a specified BERT model.
    * ----------------{Returns}---------------
        * : digits ::pd.Series | A pandas Series containing the embedded tokenized text data
    * ----------------{Params}----------------
        * : digits ::pd.Series | The input pandas Series containing the tokenized text data to be embedded
        * : bert_model ::transformers.PreTrainedModel | The BERT model to be used for embedding
    * ----------------{Usage}-----------------
        * >>> from transformers import AutoModel
        * >>> bert_model = AutoModel.from_pretrained("bert-base-uncased")
        * >>> embedded_digits = embed_tokens_bert(digits, bert_model)
    * ----------------{Notes}-----------------
        * This function embeds the tokenized text data using a specified BERT model from the transformers library. The resulting embeddings can be used for various natural language processing tasks.
    """
    import torch

    digits = digits.apply(lambda x: umap_reshape(bert_model(x)[0].detach().numpy()))
    return digits


def clean_text(text):
    """
    * type-def ::(str) -> str
    * ---------------{Function}---------------
        * Cleans a given text string by removing non-alphanumeric characters.
    * ----------------{Returns}---------------
        * : cleaned_text ::str | The cleaned text string
    * ----------------{Params}----------------
        * : text ::str | The input text string to be cleaned
    * ----------------{Usage}-----------------
        * >>> text = "Hello, world! This is an example."
        * >>> cleaned_text = clean_text(text)
    * ----------------{Notes}-----------------
        * This function cleans a given text string by removing non-alphanumeric characters using regular expressions. It can be useful for preprocessing text data before tokenization and further analysis.
    """
    import re

    text = re.sub("[^A-Za-z0-9]+", " ", text)
    return text


def np_topk(data, k):
    """
    * type-def ::(np.ndarray, int) -> np.ndarray
    * ---------------{Function}---------------
        * Returns the indices of the top k values in a NumPy array.
    * ----------------{Returns}---------------
        * : top_k_indices ::np.ndarray | The indices of the top k values in the input array
    * ----------------{Params}----------------
        * : data ::np.ndarray | The input NumPy array
        * : k ::int | The number of top values to return
    * ----------------{Usage}-----------------
        * >>> data = np.array([3, 1, 4, 2, 5])
        * >>> top_k_indices = np_topk(data, 3)
    * ----------------{Notes}-----------------
        * This function finds the indices of the top k values in a NumPy array. It can be used for tasks such as selecting the most important features or the highest-scoring items.
    """
    return data.argsort()[-k:][::-1]


def x_only_train_test_split(data, test_size=0.33):
    """
    * type-def ::(np.ndarray, float) -> Tuple[np.ndarray, np.ndarray]
    * ---------------{Function}---------------
        * Splits the input data into train and test sets without considering target labels.
    * ----------------{Returns}---------------
        * : X_train ::np.ndarray | The training set
        * : X_test ::np.ndarray | The test set
    * ----------------{Params}----------------
        * : data ::np.ndarray | The input data to be split
        * : test_size ::float | The proportion of the input data to include in the test set
    * ----------------{Usage}-----------------
        * >>> data = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])
        * >>> X_train, X_test = x_only_train_test_split(data, test_size=0.4)
    * ----------------{Notes}-----------------
        * This function splits the input data into train and test sets without considering target labels. It can be used for unsupervised learning tasks or when target labels are not available.
    """
    from sklearn.model_selection import train_test_split

    X_train, X_test, _, _ = train_test_split(
        data, np.zeros(data.shape[0]), test_size=test_size, random_state=42
    )
    return X_train, X_test


def pd_get_dummies_concat(source_df, column, prefix=None, drop=True):
    """
    * type-def ::(pd.DataFrame, str, Optional[str], bool) -> pd.DataFrame
    * ---------------{Function}---------------
        * Creates one-hot encoded dummy variables for a specified DataFrame column and concatenates them to the original DataFrame.
    * ----------------{Returns}---------------
        * : result ::pd.DataFrame | The modified DataFrame with one-hot encoded dummy variables
    * ----------------{Params}----------------
        * : source_df ::pd.DataFrame | The input DataFrame
        * : column ::str | The column name for which to create one-hot encoded dummy variables
        * : prefix ::Optional[str] | The prefix to use for the new one-hot encoded columns (default: None)
        * : drop ::bool | Whether to drop the original column after creating one-hot encoded dummy variables (default: True)
    * ----------------{Usage}-----------------
        * >>> data = {"A": ["a", "b", "a"], "B": ["b", "a", "c"]}
        * >>> df = pd.DataFrame(data)
        * >>> result_df = pd_get_dummies_concat(df, "A", prefix="A")
    * ----------------{Notes}-----------------
        * This function creates one-hot encoded dummy variables for a specified DataFrame column and concatenates them to the original DataFrame. It can be used to convert categorical data into a numerical format for machine learning algorithms.
    """
    result = pd.concat(
        [source_df, pd.get_dummies(source_df[column], prefix=prefix)],
        axis=1,
        sort=False,
    )
    if drop:
        result.drop(column, axis=1, inplace=True)
    return result


def pd_timed_slice(
    df,
    timeseries,
    week=0,
    day=0,
    hour=0,
    minute=0,
    second=0,
    millisecond=0,
    microsecond=0,
):
    """
    * type-def ::(pd.DataFrame, str, int, int, int, int, int, int, int) -> pd.DataFrame
    * ---------------{Function}---------------
        * Slices a DataFrame based on a specified time range.
    * ----------------{Returns}---------------
        * : sliced_df ::pd.DataFrame | The sliced DataFrame within the specified time range
    * ----------------{Params}----------------
        * : df ::pd.DataFrame | The input DataFrame
        * : timeseries ::str | The column name containing the time series data
        * : week ::int | The number of weeks to include in the time range (default: 0)
        * : day ::int | The number of days to include in the time range (default: 0)
        * : hour ::int | The number of hours to include in the time range (default: 0)
        * : minute ::int | The number of minutes to include in the time range (default: 0)
        * : second ::int | The number of seconds to include in the time range (default: 0)
        * : millisecond ::int | The number of milliseconds to include in the time range (default: 0)
        * : microsecond ::int | The number of microseconds to include in the time range (default: 0)
    * ----------------{Usage}-----------------
        * >>> data = {"time": pd.date_range("2020-01-01", periods=5, freq="D"), "value": [1, 2, 3, 4, 5]}
        * >>> df = pd.DataFrame(data)
        * >>> sliced_df = pd_timed_slice(df, "time", day=2)
    * ----------------{Notes}-----------------
        * This function slices a DataFrame based on a specified time range. It can be used to filter data for time series analysis, anomaly detection, or other tasks requiring a specific time window.
    """
    import datetime

    df[timeseries] = pd.to_datetime(df[timeseries])
    range_max = df[timeseries].max()
    range_min = range_max - datetime.timedelta(
        weeks=week,
        days=day,
        hours=hour,
        minutes=minute,
        seconds=second,
        milliseconds=millisecond,
        microseconds=microsecond,
    )
    sliced_df = df[(df[timeseries] >= range_min) & (df[timeseries] <= range_max)]
    return sliced_df
